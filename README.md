# Análise comparativa de algoritmos de classificação de texto

Este projeto explora e compara o desempenho do BERT (Bidirectional Encoder Representations from Transformers) com algoritmos clássicos de classificação de texto (como Árvore de Decisão, XGBoost, Naive Bayes e SVM), bem como com arquiteturas de redes neurais, como Rede Neural Tradicional (MLP) e LSTM.

## Bases Utilizadas:
* OlistKaggle: base em português com classificação binária
* Fake.br: base em português com classificação binária
* Fake.br Multi: base em português com classificação multiclasse
* Sentihood: base em inglês com classificação binária
* NewsKaggle: base em inglês com classificação multiclasse

Os códigos estão separados por pastas das bases.


# Comparative Analysis of Text Classification Algorithms

This project explores and compares the performance of BERT (Bidirectional Encoder Representations from Transformers) with classical text classification algorithms (such as Decision Tree, XGBoost, Naive Bayes, and SVM), as well as with neural network architectures, such as Traditional Neural Network (MLP) and LSTM.

## Datasets Used:

* OlistKaggle: Portuguese dataset with binary classification
* Fake.br: Portuguese dataset with binary classification
* Fake.br Multi: Portuguese dataset with multiclass classification
* Sentihood: English dataset with binary classification
* NewsKaggle: English dataset with multiclass classification

The code is organized into folders by dataset.
